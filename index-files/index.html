<!DOCTYPE HTML>

<html xmlns="http://www.w3.org/1999/xhtml">

<html>

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
 
<p>

<h3>DATA SCIENCE SPECIALISM</h3>

<h3>MODULE 8 - PRACTICAL MACHINE LEARNING</h3><br>

<h2>Investigation into qualitative activity recognition information from a dumbbell lifting exercise and the predictive ability of a boosting machine learning algorithm</h1>

<h3>AUTHOR: FE YOUNG</h3>

<h3>DATE: 2015 Nov 21</h3>

<hr><br>

<b>ABSTRACT</b><br><br>

Velloso (2013) investigated whether machine learning algorithms could accurately detect erroneous methods of lifting a dumbbell.<br><br>

Following on from their research this analysis performs a predicton using the boosting machine learning algorithm rather than best fit Random Forest approach.<br><br>

The predictive ability of Model One generated an overall accuracy was 0.96 and removing eight zero influence predictors reduced Model Two to an overall accuracy to 0.95.  Model One was used to evaluate the prediction accuracy information located in the validation dataset.  Model One correctly identified all 20 validation cases.<br><br>

<hr><br>

<b>INTRODUCTION</b><br><br>

The six male test subjects were of 20 - 28 years of age and inexperienced in dumbbell weight lifting exercises.  The dumbbell weighed 1.25 kg.<br><br>

Each subject performed a set of 10 repetitions of a unilateral dumbbell bicep curl in five different ways.  Class A corresponded to the correct execution of the exercise while methods B through E corresponded to common dumbbell lifting mistakes namely (B) throwing the elbow to the front; (C) dumbbell lifted halfway; (D) dumbbell lowered halfway; (E) throwing hips to the front.<br><br>

The question addressed in this report is can a machine learning model correctly identify 20 validation cases?<br><br>

<hr><br>

<b>METHODOLOGY</b><br><br>

<b>Loading R packages:</b>

<pre class = "r"><code>
```{R preprocessing, cache = TRUE}
	##1 - loading libraries
	library(caret); library(ggplot2); library(data.table); library(plyr); library(dplyr); library(reshape2);
	library(ggplot2); library(knitr); library(rmarkdown); library(YaleToolkit)

	##NOTE - for knitr/rmarkdown to work in RCONSOLE you are required to download the PANDOC package available online at: http://pandoc.org/installing.html
```</pre></code>

Loading required package: lattice<br>
Loading required package: ggplot2<br>
data.table 1.9.6  For help type ?data.table or https://github.com/Rdatatable/data.table/wiki<br>
The fastest way to learn (by data.table authors): https://www.datacamp.com/courses/data-analysis-the-data-table-way<br><br>

Attaching package: dplyr<br><br>

The following objects are masked from package:plyr:<br><br>

    arrange, count, desc, failwith, id, mutate, rename,
    summarise, summarize<br><br>

The following objects are masked from package:data.table:<br><br>

    between, last<br><br>

The following objects are masked from package:stats:<br><br>

    filter, lag<br><br>

The following objects are masked from package:base:<br><br>

    intersect, setdiff, setequal, union<br><br><br>


Attaching package: reshape2<br><br>

The following objects are masked from package:data.table:<br><br>

    <dt>dcast, melt</dt><br><br>

Loading required package: grid<br><br><br>

<b>What hardware/software combination did I use for this analysis?</b>

<pre class = "r"><code>
```{r session info, cache = TRUE}

	##2 - what hardware/software is this analysis using?

	sessionInfo()
```
</pre></code>

R version 3.2.2 (2015-08-14)<br>
Platform: x86_64-w64-mingw32/x64 (64-bit)<br>
Running under: Windows 8 x64 (build 9200)<br><br>

locale:<br>
[1] LC_COLLATE=English_United Kingdom.1252 <br>
[2] LC_CTYPE=English_United Kingdom.1252  <br> 
[3] LC_MONETARY=English_United Kingdom.1252<br>
[4] LC_NUMERIC=C      <br>                     
[5] LC_TIME=English_United Kingdom.1252  <br><br>  

attached base packages:<br>
[1] grid &nbsp;&nbsp;&nbsp;&nbsp;      stats   &nbsp;&nbsp;&nbsp;&nbsp;   graphics &nbsp;&nbsp;&nbsp;&nbsp;  grDevices utils  &nbsp;&nbsp;&nbsp;&nbsp;    datasets &nbsp;&nbsp;&nbsp;&nbsp;  methods <br> 
[8] base     <br><br>

other attached packages:<br>
 [1] YaleToolkit_4.2.2&nbsp;&nbsp;&nbsp;&nbsp;  rmarkdown_0.8   &nbsp;&nbsp;&nbsp;&nbsp;   knitr_1.11 <br>      
 [4] reshape2_1.4.1 &nbsp;&nbsp;&nbsp;&nbsp;    dplyr_0.4.3    &nbsp;&nbsp;&nbsp;&nbsp;    plyr_1.8.3  <br>     
 [7] data.table_1.9.6&nbsp;&nbsp;&nbsp;&nbsp;   caret_6.0-58  &nbsp;&nbsp;&nbsp;&nbsp;     ggplot2_1.0.1 <br>   
[10] lattice_0.20-33  <br><br>

loaded via a namespace (and not attached):<br>
 [1] Rcpp_0.12.1 &nbsp;&nbsp;&nbsp;&nbsp;        nloptr_1.0.4     &nbsp;&nbsp;&nbsp;&nbsp;   iterators_1.0.8   <br>
 [4] tools_3.2.2  &nbsp;&nbsp;&nbsp;&nbsp;       digest_0.6.8   &nbsp;&nbsp;&nbsp;&nbsp;     lme4_1.1-9     <br>   
 [7] nlme_3.1-122  &nbsp;&nbsp;&nbsp;&nbsp;      gtable_0.1.2   &nbsp;&nbsp;&nbsp;&nbsp;     mgcv_1.8-7    <br>    
[10] Matrix_1.2-2  &nbsp;&nbsp;&nbsp;&nbsp;      foreach_1.4.3  &nbsp;&nbsp;&nbsp;&nbsp;     DBI_0.3.1    <br>     
[13] parallel_3.2.2   &nbsp;&nbsp;&nbsp;&nbsp;   SparseM_1.7   &nbsp;&nbsp;&nbsp;&nbsp;      proto_0.3-10 <br>     
[16] stringr_1.0.0  &nbsp;&nbsp;&nbsp;&nbsp;     MatrixModels_0.4-1&nbsp;&nbsp;&nbsp;&nbsp;  stats4_3.2.2   <br>   
[19] nnet_7.3-11   &nbsp;&nbsp;&nbsp;&nbsp;      R6_2.1.1      &nbsp;&nbsp;&nbsp;&nbsp;      minqa_1.2.4   <br>    
[22] car_2.1-0     &nbsp;&nbsp;&nbsp;&nbsp;      magrittr_1.5    &nbsp;&nbsp;&nbsp;&nbsp;    htmltools_0.2.6 <br>  
[25] scales_0.3.0   &nbsp;&nbsp;&nbsp;&nbsp;     codetools_0.2-14 &nbsp;&nbsp;&nbsp;&nbsp;   MASS_7.3-44   <br>    
[28] splines_3.2.2  &nbsp;&nbsp;&nbsp;&nbsp;     assertthat_0.1     pbkrtest_0.4-2   <br> 
[31] colorspace_1.2-6  &nbsp;&nbsp;&nbsp;&nbsp;  quantreg_5.19  &nbsp;&nbsp;&nbsp;&nbsp;     stringi_0.5-5 <br>    
[34] munsell_0.4.2    &nbsp;&nbsp;&nbsp;&nbsp;   chron_2.3-47<br><br><br>

<b>Loading the training and validation datasets.  Exploration of the training dataset.</b>

<pre class = "r"><code>
```{loading datasets, cache = TRUE}
	##3 - loading datasets
	
	#3.1 - trainingdataset
	if(!file.exists('pml-training.csv')) {
	download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
	}
	training <- read.table("pml-training.csv", sep = ",", header = T)

	##3.2 - validation dataset
	if(!file.exists('pml-testing.csv')) {
	download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")
	}
	validation <- read.table("pml-testing.csv", sep =",", header = T)
	
		##4 - exploring datasets
		dim(training); dim(validation); str(training, list.len = 160)		
```
</pre></code>

[1] 19622&nbsp;&nbsp;   160<br>
[1]  20&nbsp;&nbsp; 160<br><br>

'data.frame'&nbsp;&nbsp;: &nbsp;&nbsp; 19622 obs. of  160 variables:<br>
 $ X&nbsp;&nbsp;:&nbsp;&nbsp; int  1 2 3 4 5 6 7 8 9 10 ...<br>
 $ user_name&nbsp;&nbsp;: &nbsp;&nbsp; Factor w/ 6 levels "adelmo","carlitos",..: 2 2 2 2 2 2 2 2 2 2 ...<br>
 $ raw_timestamp_part_1&nbsp;&nbsp;: &nbsp;&nbsp;int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...<br>
 $ raw_timestamp_part_2&nbsp;&nbsp;: &nbsp;&nbsp; int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...<br>
 $ cvtd_timestamp&nbsp;&nbsp;: &nbsp;&nbsp;Factor w/ 20 levels "02/12/2011 13:32",..: 9 9 9 9 9 9 9 9 9 9 ...<br>
 $ new_window &nbsp;&nbsp;: &nbsp;&nbsp;Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 1 1 1 ...<br>
 $ num_window &nbsp;&nbsp;: &nbsp;&nbsp; int  11 11 11 12 12 12 12 12 12 12 ...<br>
 $ roll_belt             &nbsp;&nbsp;: &nbsp;&nbsp; num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...<br>
 $ pitch_belt            &nbsp;&nbsp;: &nbsp;&nbsp; num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...<br>
 $ yaw_belt              &nbsp;&nbsp;: &nbsp;&nbsp; num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...<br>
 ...<br><br>

<b>The dataset consists of 19622 rows with 160 columns.  Examination of the dataset concluded that 106 columns could be removed as they contained no valid information.  These columns contained the words or abbreviations: kurtosis, mean, stddev, var, var_total, avg, skewness, max, min, new_window, num_window, amplitude.  There are no columns with missing data or zero variance.</b><br>

<pre class = "r"><code>
```{R removing columns & splitting dataset, cache = TRUE}

	##5 - removing unwanted columns and dealing with missing data
				
	##5.1 - including the words: kurtosis, mean, stddev, var, var_total, avg, skewness, max, min, new_window, num_window, amplitude and the time variables
	trainingdata <- training[c(8:10, 37:48, 60:68, 84:86, 113:124, 151:160)]
	validationdataset <- validation[c(8:10, 37:48, 60:68, 84:86, 113:124, 151:160)]
	
	##5.2 - where zeroVar = 0 AND nzv = TRUE remove columns? NOTHING TO DEAL WITH
	removezero1 <- nearZeroVar(trainingdata, saveMetrics = T)
	removezero1
```
</pre></code>

  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;                 freqRatio&nbsp;&nbsp;&nbsp;&nbsp;  percentUnique&nbsp;&nbsp;&nbsp;&nbsp;  zeroVar &nbsp;&nbsp;&nbsp;&nbsp;   nzv<br>
roll_belt         &nbsp;&nbsp;&nbsp;&nbsp;  1.101904 &nbsp;&nbsp;&nbsp;&nbsp;     6.7781062 &nbsp;&nbsp;&nbsp;&nbsp;   FALSE &nbsp;&nbsp;&nbsp;&nbsp; FALSE<br>
pitch_belt   &nbsp;&nbsp;&nbsp;&nbsp;       1.036082   &nbsp;&nbsp;&nbsp;&nbsp;   9.3772296 &nbsp;&nbsp;&nbsp;&nbsp;   FALSE&nbsp;&nbsp;&nbsp;&nbsp;  FALSE<br>
yaw_belt      &nbsp;&nbsp;&nbsp;&nbsp;      1.058480  &nbsp;&nbsp;&nbsp;&nbsp;    9.9734991 &nbsp;&nbsp;&nbsp;&nbsp;   FALSE &nbsp;&nbsp;&nbsp;&nbsp; FALSE<br>
gyros_belt_x   &nbsp;&nbsp;&nbsp;&nbsp;     1.058651  &nbsp;&nbsp;&nbsp;&nbsp;    0.7134849  &nbsp;&nbsp;&nbsp;&nbsp;  FALSE&nbsp;&nbsp;&nbsp;&nbsp;  FALSE<br>
gyros_belt_y    &nbsp;&nbsp;&nbsp;&nbsp;    1.144000  &nbsp;&nbsp;&nbsp;&nbsp;    0.3516461 &nbsp;&nbsp;&nbsp;&nbsp;   FALSE&nbsp;&nbsp;&nbsp;&nbsp;  FALSE<br>
gyros_belt_z    &nbsp;&nbsp;&nbsp;&nbsp;    1.066214  &nbsp;&nbsp;&nbsp;&nbsp;    0.8612782 &nbsp;&nbsp;&nbsp;&nbsp;   FALSE&nbsp;&nbsp;&nbsp;&nbsp;  FALSE<br>
accel_belt_x  &nbsp;&nbsp;&nbsp;&nbsp;      1.055412  &nbsp;&nbsp;&nbsp;&nbsp;    0.8357966 &nbsp;&nbsp;&nbsp;&nbsp;   FALSE &nbsp;&nbsp;&nbsp;&nbsp; FALSE<br>
accel_belt_y   &nbsp;&nbsp;&nbsp;&nbsp;     1.113725 &nbsp;&nbsp;&nbsp;&nbsp;     0.7287738  &nbsp;&nbsp;&nbsp;&nbsp;  FALSE &nbsp;&nbsp;&nbsp;&nbsp; FALSE<br>
accel_belt_z   &nbsp;&nbsp;&nbsp;&nbsp;     1.078767  &nbsp;&nbsp;&nbsp;&nbsp;    1.5237998 &nbsp;&nbsp;&nbsp;&nbsp;   FALSE&nbsp;&nbsp;&nbsp;&nbsp;  FALSE<br>
magnet_belt_x  &nbsp;&nbsp;&nbsp;&nbsp;     1.090141    &nbsp;&nbsp;&nbsp;&nbsp;  1.6664968  &nbsp;&nbsp;&nbsp;&nbsp;  FALSE &nbsp;&nbsp;&nbsp;&nbsp; FALSE<br>
...<br><br>


<pre class = "r"><code>
```{r whatis, cache = TRUE}

	##5.3 - is there any missing data to impute?  NOTHING TO DEAL WITH
	whatis(trainingdata)
```
</pre></code>

variable.name        &nbsp;&nbsp;&nbsp;&nbsp; type &nbsp;&nbsp;&nbsp;&nbsp; missing &nbsp;&nbsp;&nbsp;&nbsp; distinct.values &nbsp;&nbsp;&nbsp;&nbsp; precision    &nbsp;&nbsp;&nbsp;&nbsp;       min     &nbsp;&nbsp;&nbsp;&nbsp;     max<br>
&nbsp; &nbsp;1        roll_belt  &nbsp;&nbsp;&nbsp;&nbsp;   numeric   &nbsp;&nbsp;&nbsp;&nbsp;    0       &nbsp;&nbsp;&nbsp;&nbsp;     1330   &nbsp;&nbsp;&nbsp;&nbsp;  1e-02    &nbsp;&nbsp;&nbsp;&nbsp;     -28.9       &nbsp;&nbsp;&nbsp;&nbsp;   162<br>
&nbsp; &nbsp;2        pitch_belt   &nbsp;&nbsp;&nbsp;&nbsp;  numeric   &nbsp;&nbsp;&nbsp;&nbsp;    0         &nbsp;&nbsp;&nbsp;&nbsp;   1840   &nbsp;&nbsp;&nbsp;&nbsp;  1e-02    &nbsp;&nbsp;&nbsp;&nbsp;     -55.8      &nbsp;&nbsp;&nbsp;&nbsp;   60.3<br>
&nbsp; &nbsp;3        yaw_belt &nbsp;&nbsp;&nbsp;&nbsp;    numeric    &nbsp;&nbsp;&nbsp;&nbsp;   0         &nbsp;&nbsp;&nbsp;&nbsp;   1957   &nbsp;&nbsp;&nbsp;&nbsp;  1e-02      &nbsp;&nbsp;&nbsp;&nbsp;    -180       &nbsp;&nbsp;&nbsp;&nbsp;   179<br>
&nbsp; &nbsp;4        gyros_belt_x &nbsp;&nbsp;&nbsp;&nbsp;    numeric   &nbsp;&nbsp;&nbsp;&nbsp;    0       &nbsp;&nbsp;&nbsp;&nbsp;      140   &nbsp;&nbsp;&nbsp;&nbsp;  1e-02    &nbsp;&nbsp;&nbsp;&nbsp;     -1.04   &nbsp;&nbsp;&nbsp;&nbsp;      2.22<br>
&nbsp; &nbsp;5   gyros_belt_y     &nbsp;&nbsp;&nbsp;&nbsp;numeric &nbsp;&nbsp;&nbsp;&nbsp;      0           &nbsp;&nbsp;&nbsp;&nbsp;   69  &nbsp;&nbsp;&nbsp;&nbsp;   1e-02    &nbsp;&nbsp;&nbsp;&nbsp;     -0.64    &nbsp;&nbsp;&nbsp;&nbsp;     0.64<br>
&nbsp; &nbsp;6    gyros_belt_z   &nbsp;&nbsp;&nbsp;&nbsp;  numeric   &nbsp;&nbsp;&nbsp;&nbsp;    0         &nbsp;&nbsp;&nbsp;&nbsp;    169   &nbsp;&nbsp;&nbsp;&nbsp;  1e-02    &nbsp;&nbsp;&nbsp;&nbsp;     -1.46    &nbsp;&nbsp;&nbsp;&nbsp;     1.62<br>
&nbsp; &nbsp;7    accel_belt_x   &nbsp;&nbsp;&nbsp;&nbsp;  numeric   &nbsp;&nbsp;&nbsp;&nbsp;    0         &nbsp;&nbsp;&nbsp;&nbsp;    164   &nbsp;&nbsp;&nbsp;&nbsp;  1e+00     &nbsp;&nbsp;&nbsp;&nbsp;     -120      &nbsp;&nbsp;&nbsp;&nbsp;     85<br>
&nbsp; &nbsp;8    accel_belt_y  &nbsp;&nbsp;&nbsp;&nbsp;   numeric    &nbsp;&nbsp;&nbsp;&nbsp;   0          &nbsp;&nbsp;&nbsp;&nbsp;   143   &nbsp;&nbsp;&nbsp;&nbsp;  1e+00   &nbsp;&nbsp;&nbsp;&nbsp;        -69     &nbsp;&nbsp;&nbsp;&nbsp;     164<br>
&nbsp; &nbsp;9   accel_belt_z   &nbsp;&nbsp;&nbsp;&nbsp;  numeric    &nbsp;&nbsp;&nbsp;&nbsp;   0           &nbsp;&nbsp;&nbsp;&nbsp;  299   &nbsp;&nbsp;&nbsp;&nbsp;  1e+00    &nbsp;&nbsp;&nbsp;&nbsp;      -275     &nbsp;&nbsp;&nbsp;&nbsp;     105<br>
10   magnet_belt_x   &nbsp;&nbsp;&nbsp;&nbsp;  numeric    &nbsp;&nbsp;&nbsp;&nbsp;   0        &nbsp;&nbsp;&nbsp;&nbsp;     327    &nbsp;&nbsp;&nbsp;&nbsp; 1e+00    &nbsp;&nbsp;&nbsp;&nbsp;       -52      &nbsp;&nbsp;&nbsp;&nbsp;    485<br>
...<br><br>

<b>It was decided to split the dataset into two randomly selected pieces using the createDataPartition command because of the large dataset size.  The two pieces: 60% (11767 rows) for model training and 40% (7846 rows) for model testing were chosen by trial and error.  The training model provided evidence that model accuracy increased as the size of training dataset was increased, but was constrained by computing power.<br><br>

A validation dataset has been supplied containing 20 rows in order to fulfil the project requirement for this Data Science Specialism module.  One  point per row will be awarded for each correctly predicted answer by the generated model.<br></b>

<pre class = "r"><code>
```{R splitting, cache = TRUE}

	##6 - splitting the dataset 70:30 training:testing
	split1 <- createDataPartition(y = trainingdata$classe, p = 0.6, list = FALSE)
	trainingdataset <- trainingdata[split1, ]
	testingdataset <- trainingdata[-split1, ]
	dim(trainingdataset); dim(testingdataset)
```
</pre></code>

[1] 11776    49<br>
[1] 7846   49<br><br><br>

<b>The training model instructions required that the classe (A - E) variable was to be predicted by the model.  To train the model the classe variable had to be removed so not to predict itself.</b><br>

<pre class = "r"><code>
```{R premodelling, cache = TRUE}
	##7 - classe ~ user_name + all variables INCLUDING PREPROCESSING
	namestraining <- names(trainingdataset[c(-49)])
	form <- as.formula(paste("classe~", paste(namestraining, collapse = "+"), sep = ""))
```
</pre></code><br>

<b>The choice of training model was related to its computational RAM (random access memory) expense, the time available to complete the module project analysis submission and the defined accuracy of the model compared to others as described by Jeff Leek in the video lecture on Boosting (see References).  This led to the selection of the boosting model - command "gbm". <br><br>

Preprocessing of the training dataset was performed at the same time as model training and it centred and scaled the all variables.  If any other preprocessing commands were added to the model the computer produced a BSoD (blue screen of death).<br><br>

The boosting model on the training dataset was run several times and each time there was a slightly different accuracy output so for reproducibility a seed was set, number 1258 was used.</b><br>

<pre class = "r"><code>
```{R training model, cache = TRUE}
	##8 - Model 1
	set.seed(1258)
	model1 <- train(form, data = trainingdataset, preProcess = c("scale", "center"), method = "gbm", verbose = F)
```
</pre></code><br>

<b>On completion of the model it was noted that eight predictors had no model influence and were removed from the training dataset.  The predictors are accel_belt_x, accel_belt_y, pitch_arm, gyros_arm_z, accel_arm_y, yaw_dumbbell, yaw_forearm and gyros_forearm_y.</b><br>

<pre class = "r"><code>
```{r removal}
	##9 - removing additional variables
	trainingdataset2 <- trainingdataset[c(1:6, 9:13, 15:17, 19, 21:26, 28:38, 40, 42:49)]
	dim(trainingdataset2)
```
</pre></code>

[1] 11776&nbsp;&nbsp; 41<br><br><br>

<b>The model was run again without these eight predictors for the purpose of cross validation with 50% of the original training dataset rows randomly chosen.  Would the model accuracy improved without these eight variables?</b><br>

<pre class = "r"><code>
```{r splitting 2}
	##10 - splitting the training dataset into 2 pieces 50:50
	split2 <- createDataPartition(y = trainingdataset2$classe, p = 0.5, list = FALSE)
	trainingdataset3 <- trainingdataset2[split2, ]

	##11 - removing the classe variable
	namestraining <- names(trainingdataset3[c(-41)])
	form2 <- as.formula(paste("classe~", paste(namestraining, collapse = "+"), sep = ""))

	##12 - MODEL 2
	model2 <- train(form2, data = trainingdataset3, preProcess = c("scale", "center"), method = "gbm", verbose = F)
```
</pre></code><br>

<b>On satisfactory training of the final model it was used to predict the validation dataset.</b><br><br>

<hr><br>

<b>RESULTS</b><br><br>

<b>The results from the Model One and its predictive accuracy on the testing dataset:</b><br>

<pre class = "r"><code>
```{r boosting model 1, cache = TRUE}

	##13 - Model 1 results
	print(model1$finalModel)
	plot(model1)
	summary(model1)
	prediction1 <- predict(model1, testingdataset)
	qplot(prediction1, colour = classe, fill = classe, data = testingdataset, main = "Predicting the testing dataset by Model 1\n", ylab = "Count\n")
	confusionMatrix(testingdataset$classe, predict(model1, testingdataset))
```
</pre></code><br>

A gradient boosted model with multinomial loss function.<br>
150 iterations were performed.<br>
There were 48 predictors of which 39 had non-zero influence.<br>

<img src = "graphic1.png"><br>

summary(model1)<br><br>

<img src = "graphic2.png"><br><br>

 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;               var  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  rel.inf<br>
roll_belt&nbsp;&nbsp; 21.83897809<br>
pitch_forearm &nbsp;&nbsp;11.36695394<br>
yaw_belt  &nbsp;&nbsp;&nbsp;&nbsp; 8.97865050<br>
magnet_dumbbell_z &nbsp;&nbsp;&nbsp;&nbsp; 6.57919727<br>
magnet_dumbbell_y &nbsp;&nbsp;&nbsp;&nbsp; 5.74142037<br>
roll_forearm   &nbsp;&nbsp;&nbsp;&nbsp; 5.14858798<br>
magnet_belt_z      &nbsp;&nbsp;&nbsp;&nbsp; 3.99357158<br>
pitch_belt      &nbsp;&nbsp;&nbsp;&nbsp; 3.37862292<br>
accel_forearm_x   &nbsp;&nbsp;&nbsp;&nbsp;3.31485298<br>
...<br>

<img src = "graphic3.png"><br><br>

Confusion Matrix and Statistics<br><br>

          Reference<br>
Prediction    A  &nbsp;&nbsp;&nbsp;&nbsp;  B  &nbsp;&nbsp;&nbsp;&nbsp;  C   &nbsp;&nbsp;&nbsp;&nbsp; D   &nbsp;&nbsp;&nbsp;&nbsp; E<br>
&nbsp;&nbsp;&nbsp;&nbsp;       A &nbsp;&nbsp;&nbsp;&nbsp;2204  &nbsp;&nbsp;&nbsp;&nbsp; 19 &nbsp;&nbsp;&nbsp;&nbsp;   6  &nbsp;&nbsp;&nbsp;&nbsp;  3  &nbsp;&nbsp;&nbsp;&nbsp;  0<br>
&nbsp;&nbsp;&nbsp;&nbsp;       B  &nbsp;&nbsp;&nbsp;&nbsp; 55&nbsp;&nbsp;&nbsp;&nbsp; 1428  &nbsp;&nbsp;&nbsp;&nbsp; 31  &nbsp;&nbsp;&nbsp;&nbsp;  2  &nbsp;&nbsp;&nbsp;&nbsp;  2<br>
&nbsp;&nbsp;&nbsp;&nbsp;       C  &nbsp;&nbsp;&nbsp;&nbsp;  0 &nbsp;&nbsp;&nbsp;&nbsp;  44 &nbsp;&nbsp;&nbsp;&nbsp;1300  &nbsp;&nbsp;&nbsp;&nbsp; 24  &nbsp;&nbsp;&nbsp;&nbsp;  0<br>
&nbsp;&nbsp;&nbsp;&nbsp;      D  &nbsp;&nbsp;&nbsp;&nbsp;  1  &nbsp;&nbsp;&nbsp;&nbsp;  4  &nbsp;&nbsp;&nbsp;&nbsp; 42 &nbsp;&nbsp;&nbsp;&nbsp;1228  &nbsp;&nbsp;&nbsp;&nbsp; 11<br>
&nbsp;&nbsp;&nbsp;&nbsp;    E  &nbsp;&nbsp;&nbsp;&nbsp;  2  &nbsp;&nbsp;&nbsp;&nbsp; 27  &nbsp;&nbsp;&nbsp;&nbsp; 18  &nbsp;&nbsp;&nbsp;&nbsp; 21 &nbsp;&nbsp;&nbsp;&nbsp;1374<br><br>

Overall Statistics<br><br>
                                          
               Accuracy : 0.9602    <br>      
                 95% CI : (0.9557, 0.9645)<br>
    No Information Rate : 0.2883    <br>      
    P-Value [Acc > NIR] : < 2.2e-16  <br><br>     
                                          
                  Kappa : 0.9497   <br>       
 Mcnemar's Test P-Value : 1.922e-12 <br><br>      

Statistics by Class:<br><br>

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;                 Class: A &nbsp;&nbsp;&nbsp;&nbsp;Class: B &nbsp;&nbsp;&nbsp;&nbsp;Class: C &nbsp;&nbsp;&nbsp;&nbsp;Class: D&nbsp;&nbsp;&nbsp;&nbsp; Class: E<br>
Sensitivity &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    &nbsp;&nbsp;&nbsp;&nbsp;       0.9744   &nbsp;&nbsp;0.9382  &nbsp;&nbsp; 0.9306  &nbsp;&nbsp; 0.9609 &nbsp;&nbsp;  0.9906<br>
Specificity &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;        0.9950 &nbsp;&nbsp;  0.9858  &nbsp;&nbsp; 0.9895 &nbsp;&nbsp;  0.9912  &nbsp;&nbsp; 0.9895<br>
Pos Pred Value &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;       0.9875  &nbsp;&nbsp; 0.9407 &nbsp;&nbsp;  0.9503  &nbsp;&nbsp; 0.9549  &nbsp;&nbsp; 0.9528<br>
Neg Pred Value&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;       0.9897 &nbsp;&nbsp;  0.9851  &nbsp;&nbsp; 0.9850 &nbsp;&nbsp;  0.9924  &nbsp;&nbsp; 0.9980<br>
Prevalence   &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;        0.2883  &nbsp;&nbsp; 0.1940  &nbsp;&nbsp; 0.1781 &nbsp;&nbsp;  0.1629  &nbsp;&nbsp; 0.1768<br>
Detection Rate  &nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;     0.2809 &nbsp;&nbsp;  0.1820  &nbsp;&nbsp; 0.1657  &nbsp;&nbsp; 0.1565 &nbsp;&nbsp;  0.1751<br>
Detection Prevalence &nbsp;&nbsp;&nbsp;&nbsp;  0.2845 &nbsp;&nbsp;  0.1935  &nbsp;&nbsp; 0.1744 &nbsp;&nbsp;  0.1639  &nbsp;&nbsp; 0.1838<br>
Balanced Accuracy   &nbsp;&nbsp;&nbsp;&nbsp;   0.9847 &nbsp;&nbsp;  0.9620 &nbsp;&nbsp;  0.9600  &nbsp;&nbsp; 0.9760  &nbsp;&nbsp; 0.9900<br><br>

<b>The resultant statistical output was examined.  The overall accuracy of the model was 0.96.  The positive predictive value (PPV) was over 0.95 for classes A, C to E and class B above 0.94 whereas the negative predictive value (NPV) was above 0.98 for all classes.<br><br>

The results for Model Two (below) demonstrated that removing the eight predictors reduced the accuracy of the boosting model from 0.96 to 0.95.  The PPV was reduced for four of the classes to 0.94 but class B reduced to 0.91.  For NPV all class values were above 0.98.  As the accuracy of the model dropped without these eight predictors the first model was chosen to make predictions for the validation dataset.</b><br>

<pre class = "r"><code>
```{r boosting model 2, cache = TRUE}
	##14 - Model 2 results
	print(model2$finalModel)
	summary(model2)
	prediction2 <- predict(model2, testingdataset)
	qplot(prediction2, colour = classe, fill = classe, data = testingdataset, main = "Predicting the testing dataset by Model 2\n", ylab = "Count\n")
	confusionMatrix(testingdataset$classe, predict(model2, testingdataset))
```
</pre></code>

A gradient boosted model with multinomial loss function.<br>
150 iterations were performed.<br>
There were 40 predictors of which 38 had non-zero influence.<br><br>

summary(model2)<br><br>

<img src = "graphic5.png"><br><br>

                       &nbsp;&nbsp;&nbsp;&nbsp;         var  &nbsp;&nbsp;&nbsp;&nbsp;   rel.inf
roll_belt        &nbsp;&nbsp;&nbsp;&nbsp; 20.41139035<br>
pitch_forearm     &nbsp;&nbsp;&nbsp;&nbsp; 11.13230517<br>
yaw_belt           &nbsp;&nbsp;&nbsp;&nbsp;  8.57040521<br>
magnet_dumbbell_z &nbsp;&nbsp;&nbsp;&nbsp; 7.38331461<br>
magnet_dumbbell_y &nbsp;&nbsp;&nbsp;&nbsp; 5.62204502<br>
roll_forearm       &nbsp;&nbsp;&nbsp;&nbsp; 4.81180232<br>
pitch_belt         &nbsp;&nbsp;&nbsp;&nbsp;  4.22906941<br>
magnet_belt_z      &nbsp;&nbsp;&nbsp;&nbsp; 4.10488396<br>
gyros_belt_z     &nbsp;&nbsp;&nbsp;&nbsp;  3.48826302<br>
accel_forearm_x   &nbsp;&nbsp;&nbsp;&nbsp; 2.93847954<br>
...<br>

<img src = "graphic6.png"><br><br>

Confusion Matrix and Statistics<br><br>

  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;       Reference<br>
Prediction    A  &nbsp;&nbsp;&nbsp;&nbsp;  B  &nbsp;&nbsp;&nbsp;&nbsp;  C   &nbsp;&nbsp;&nbsp;&nbsp; D  &nbsp;&nbsp;&nbsp;&nbsp;  E<br>
      &nbsp;&nbsp;&nbsp;&nbsp;   A &nbsp;&nbsp;&nbsp;&nbsp;2202  &nbsp;&nbsp;&nbsp;&nbsp; 17  &nbsp;&nbsp;&nbsp;&nbsp; 10   &nbsp;&nbsp;&nbsp;&nbsp; 3   &nbsp;&nbsp;&nbsp;&nbsp; 0<br>
       &nbsp;&nbsp;&nbsp;&nbsp;  B &nbsp;&nbsp;&nbsp;&nbsp;  79&nbsp;&nbsp;&nbsp;&nbsp; 1385  &nbsp;&nbsp;&nbsp;&nbsp; 48   &nbsp;&nbsp;&nbsp;&nbsp; 3  &nbsp;&nbsp;&nbsp;&nbsp;  3<br>
    &nbsp;&nbsp;&nbsp;&nbsp;     C  &nbsp;&nbsp;&nbsp;&nbsp;  0  &nbsp;&nbsp;&nbsp;&nbsp; 49 &nbsp;&nbsp;&nbsp;&nbsp;1297 &nbsp;&nbsp;&nbsp;&nbsp;  21   &nbsp;&nbsp;&nbsp;&nbsp; 1<br>
     &nbsp;&nbsp;&nbsp;&nbsp;    D  &nbsp;&nbsp;&nbsp;&nbsp;  1   &nbsp;&nbsp;&nbsp;&nbsp; 3 &nbsp;&nbsp;&nbsp;&nbsp;  56&nbsp;&nbsp;&nbsp;&nbsp; 1213  &nbsp;&nbsp;&nbsp;&nbsp; 13<br>
   &nbsp;&nbsp;&nbsp;&nbsp;      E  &nbsp;&nbsp;&nbsp;&nbsp;  5 &nbsp;&nbsp;&nbsp;&nbsp;  26  &nbsp;&nbsp;&nbsp;&nbsp; 32  &nbsp;&nbsp;&nbsp;&nbsp; 20 &nbsp;&nbsp;&nbsp;&nbsp;1359<br><br>

Overall Statistics<br><br>
                                         
               Accuracy : 0.9503<br>         
                 95% CI : (0.9453, 0.955)<br>
    No Information Rate : 0.2915         <br>
    P-Value [Acc > NIR] : < 2.2e-16      <br><br>
                                         
                  Kappa : 0.9371         <br>
 Mcnemar's Test P-Value : < 2.2e-16      <br><br>

Statistics by Class:<br><br>

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;                  Class: A &nbsp;&nbsp;&nbsp;&nbsp;Class: B&nbsp;&nbsp;&nbsp;&nbsp; Class: C &nbsp;&nbsp;&nbsp;&nbsp;Class: D &nbsp;&nbsp;&nbsp;&nbsp;Class: E<br>
Sensitivity        &nbsp;&nbsp;&nbsp;&nbsp;    0.9628 &nbsp;&nbsp;&nbsp;&nbsp;  0.9358  &nbsp;&nbsp;&nbsp;&nbsp; 0.8988 &nbsp;&nbsp;&nbsp;&nbsp;  0.9627 &nbsp;&nbsp;&nbsp;&nbsp;  0.9876<br>
Specificity         &nbsp;&nbsp;&nbsp;&nbsp;   0.9946  &nbsp;&nbsp;&nbsp;&nbsp; 0.9791  &nbsp;&nbsp;&nbsp;&nbsp; 0.9889  &nbsp;&nbsp;&nbsp;&nbsp; 0.9889 &nbsp;&nbsp;&nbsp;&nbsp;  0.9872<br>
Pos Pred Value    &nbsp;&nbsp;&nbsp;&nbsp;     0.9866 &nbsp;&nbsp;&nbsp;&nbsp;  0.9124  &nbsp;&nbsp;&nbsp;&nbsp; 0.9481  &nbsp;&nbsp;&nbsp;&nbsp; 0.9432  &nbsp;&nbsp;&nbsp;&nbsp; 0.9424<br>
Neg Pred Value      &nbsp;&nbsp;&nbsp;&nbsp;   0.9849 &nbsp;&nbsp;&nbsp;&nbsp;  0.9850 &nbsp;&nbsp;&nbsp;&nbsp;  0.9775  &nbsp;&nbsp;&nbsp;&nbsp; 0.9928  &nbsp;&nbsp;&nbsp;&nbsp; 0.9973<br>
Prevalence         &nbsp;&nbsp;&nbsp;&nbsp;    0.2915  &nbsp;&nbsp;&nbsp;&nbsp; 0.1886  &nbsp;&nbsp;&nbsp;&nbsp; 0.1839  &nbsp;&nbsp;&nbsp;&nbsp; 0.1606 &nbsp;&nbsp;&nbsp;&nbsp;  0.1754<br>
Detection Rate      &nbsp;&nbsp;&nbsp;&nbsp;   0.2807  &nbsp;&nbsp;&nbsp;&nbsp; 0.1765  &nbsp;&nbsp;&nbsp;&nbsp; 0.1653 &nbsp;&nbsp;&nbsp;&nbsp;  0.1546  &nbsp;&nbsp;&nbsp;&nbsp; 0.1732<br>
Detection Prevalence  &nbsp;&nbsp;&nbsp;&nbsp; 0.2845  &nbsp;&nbsp;&nbsp;&nbsp; 0.1935  &nbsp;&nbsp;&nbsp;&nbsp; 0.1744 &nbsp;&nbsp;&nbsp;&nbsp;  0.1639  &nbsp;&nbsp;&nbsp;&nbsp; 0.1838<br>
Balanced Accuracy    &nbsp;&nbsp;&nbsp;&nbsp;  0.9787   &nbsp;&nbsp;&nbsp;&nbsp;0.9575 &nbsp;&nbsp;&nbsp;&nbsp;  0.9439 &nbsp;&nbsp;&nbsp;&nbsp;  0.9758  &nbsp;&nbsp;&nbsp;&nbsp; 0.9874<br><br>

<b>The prediction results of the 20 validation cases using Model One:</b><br>

<pre class = "r"><code>
```{r validation predictions, cache = TRUE}

	##15 - Predictions with Model 1
	predict(model1, validationdataset)
```
</pre></code>

 [1] B A B A A E D B A A B C B A E E A B B B<br>
Levels: A B C D E<br><br>

<b>Of the 20 cases all 20 have been correctly predicted.</b><br><br>

<hr><br>

<b>REFERENCES</b><br><br>

Velloso E, Bulling A, Gellersen H, Ugulino W, Fuks H (2013) Qualitative Activity Recognition of Weight Lifting Exercises, Proceedings of the 4th International Conference in Cooperation with SIGCHI (Augmented Human 2013), Stuttgart, Germany<br><br>
	
Guillaume Bourgault & Chris W (2015) Distribution of each variable for each test subject and each class (A - E) online at https://class.coursera.org/predmachlearn-034/forum/thread?thread_id=20<br><br>

Leek J (2015) Boosting video lecture available online at https://class.coursera.org/predmachlearn-034/lecture/49<br><br>

<hr><br>

<b>APPENDICES</b><br><br>

<b>APPENDIX 1: Codebook</b><br><br>

Abbreviations for parts of the column names:-<br><br>

gyros 	<- gyroscope<br>
x 		<- x axis<br>
y 		<- y axis<br>
z 		<- z axis<br>
accel	<- accelerometer<br>
magnet	<- magnetometer<br><br>

Meaning of classe headings:-<br><br>

(A) correct execution of the exercise<br>
(B) throwing the elbow to the front<br>
(C) dumbbell lifted halfway<br>
(D) dumbbell lowered halfway<br>
(E) throwing hips to the front<br><br>

Position of sensors:-<br><br>

belt		around the waist<br>
arm			around the upper arm<br>
forearm		around the lower arm<br>
dumbbell	on the end of the dumbbell<br><br>

<b>APPENDIX 2</b><br><br>

The information for this project comes from these sources:<br><br>

http://groupware.les.inf.puc-rio.br/har<br><br></a>

The links to the datasets are:<br><br>

Entire dataset:<br><br>

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv<br><br>

Validation dataset: <br><br>

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv<br><br>

<b>APPENDIX 3</b><br><br>

With only 2 GB of hard disk space the html document could not be constructed within the R package.  Therefore I had to write the whole html file myself using Notepad++.  So if the result tables look a little strange and columns not correctly aligned this is the reason why.<br>

<pre class = "r"><code>
```{r html}
render("project.Rmd", html_document(), quiet = T)
```
</pre></code>

</p>

</body>

</html>
 
